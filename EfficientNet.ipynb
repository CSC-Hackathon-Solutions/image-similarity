{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Imports\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os\n",
    "from itertools import islice\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    https://towardsdatascience.com/contrastive-loss-explaned-159f2d4a87ec for more details \n",
    "\"\"\"\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, distance, label):\n",
    "        loss_contrastive = torch.mean(label * torch.pow(distance, 2) +\n",
    "                                      (1 - label) * torch.pow(torch.clamp(self.margin - distance, min=0), 2))\n",
    "\n",
    "        return loss_contrastive\n",
    "\n",
    "\"\"\"\n",
    "    https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942\n",
    "\"\"\"\n",
    "\n",
    "class SiameseNetworkClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(SiameseNetworkClassifier, self).__init__()\n",
    "\n",
    "        # Replace with frozen pretrained model feature layers\n",
    "        self.pretrained = EfficientNet.from_pretrained('efficientnet-b1')\n",
    "\n",
    "        # Freeze all layers\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Create the hot layers\n",
    "        self.fc_in_features = 1280*9*9\n",
    "        self.hot = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.fc_in_features, 50)\n",
    "        )\n",
    "\n",
    "        self.threshold = torch.tensor(0.)\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, images1, images2):\n",
    "        output1 = self.pretrained.extract_features(images1)\n",
    "        output2 = self.pretrained.extract_features(images2)\n",
    "        output1 = self.hot(output1)\n",
    "        output2 = self.hot(output2)\n",
    "        return F.pairwise_distance(output1, output2)\n",
    "        \n",
    "    def update_threshold(self, loader, max_batches=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            distances = []\n",
    "            labels = []\n",
    "            for images1, images2, equals in islice(tqdm(loader, desc='Calculating threshold'), max_batches):\n",
    "                distance = self.forward(images1.to(self.device), images2.to(self.device))\n",
    "                distances.append(distance.cpu())\n",
    "                labels.append(equals)\n",
    "    \n",
    "            distances = torch.cat(distances)\n",
    "            labels = torch.cat(labels)\n",
    "            log_reg = LogisticRegression(penalty=None)\n",
    "            log_reg.fit(distances.reshape((-1, 1)), labels)\n",
    "            self.threshold = (-log_reg.intercept_ / log_reg.coef_).item()\n",
    "\n",
    "    # TODO refactor this method so we don't have to call .to(self.device) ? \n",
    "    def predict(self, images1, images2):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            images1 = images1.to(self.device)\n",
    "            images2 = images2.to(self.device)\n",
    "            distances = self.forward(images1, images2)\n",
    "            return (distances < self.threshold).int().cpu()\n",
    "\n",
    "def evaluate(model, loader, max_batches=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pos_f1 = BinaryF1Score()\n",
    "        neg_f1 = BinaryF1Score()\n",
    "        for images1, images2, label in islice(loader, max_batches):\n",
    "            distance = model.forward(images1.to(model.device), images2.to(model.device)).cpu()\n",
    "            pos_f1.update(distance < model.threshold, label)\n",
    "            neg_f1.update(distance > model.threshold, 1 - label)\n",
    "    return (pos_f1.compute() + neg_f1.compute()) / 2\n",
    "    \n",
    "def train(model, train_loader, train_threshold_loader, valid_loader, test_loader, epochs=20, lr=1e-4, max_batches=None):\n",
    "    criterion = ContrastiveLoss().to(model.device)\n",
    "    optimizer = torch.optim.Adam(model.hot.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Starting epoch {epoch}...\")\n",
    "        for images1, images2, label in islice(tqdm(train_loader, desc='Training model'), max_batches):\n",
    "            model.train()\n",
    "            images1 = images1.to(model.device)\n",
    "            images2 = images2.to(model.device)\n",
    "            label = label.to(model.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(images1, images2)\n",
    "            loss = criterion(outputs, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.update_threshold(train_threshold_loader, max_batches=max_batches)\n",
    "        \n",
    "        print(f'Epoch {epoch} | Loss:{loss.item()}')\n",
    "        print(f'Train fscore: {evaluate(model, train_loader, max_batches=max_batches)}')\n",
    "        print(f'Valid fscore: {evaluate(model, valid_loader, max_batches=max_batches)}')\n",
    "    print(f'Test fscore: {evaluate(model, test_loader, max_batches=max_batches)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(300),\n",
    "    T.CenterCrop(300),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "df_slices = list(split_dataframe(pd.read_csv('data/train.csv'), (0.9, 0.01, 0.04, 0.05)))\n",
    "\n",
    "datasets = [ImageDataset(df, transform=transform) for df in df_slices]\n",
    "loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in datasets]\n",
    "\n",
    "train_dataset, train_threshold_dataset, valid_dataset, test_dataset = datasets\n",
    "train_loader, train_threshold_loader, valid_loader, test_loader = loaders\n",
    "\n",
    "submit_df = pd.read_csv('data/submit.csv')\n",
    "submit_dataset = ImageDataset(submit_df, transform=transform)\n",
    "submit_loader = DataLoader(submit_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SiameseNetworkClassifier(device='mps')\n",
    "train(model, *loaders, epochs=10, max_batches=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Check on what's wrong with our model\n",
    "\"\"\"\n",
    "\n",
    "mislabeled(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Save predictions for the submission\n",
    "\"\"\"\n",
    "\n",
    "max_submit_id = 22661\n",
    "save_submission(model, submit_loader, max_submit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
