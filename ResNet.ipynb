{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942\n",
    "\"\"\"\n",
    "\n",
    "class SiameseNetworkClassifier(nn.Module):\n",
    "    def __init__(self, latent_space_dim=50, dropout=0.6, device='mps'):\n",
    "        super(SiameseNetworkClassifier, self).__init__()\n",
    "\n",
    "        resnet = resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.frozen = nn.Sequential(*list(resnet.children())[:-1]) \n",
    "        self.hot = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(resnet.fc.in_features, latent_space_dim)\n",
    "        )\n",
    "\n",
    "        self.threshold = torch.tensor(0.)\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, images1, images2):\n",
    "        output1 = self.hot(self.frozen(images1))\n",
    "        output2 = self.hot(self.frozen(images2))\n",
    "        return F.pairwise_distance(output1, output2)\n",
    "        \n",
    "    def update_threshold(self, loader, max_batches=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            distances = []\n",
    "            labels = []\n",
    "            for images1, images2, equals in islice(loader, max_batches):\n",
    "                distance = self.forward(images1.to(self.device), images2.to(self.device))\n",
    "                distances.append(distance.cpu())\n",
    "                labels.append(equals)\n",
    "    \n",
    "            distances = torch.cat(distances)\n",
    "            labels = torch.cat(labels)\n",
    "            log_reg = LogisticRegression(penalty=None)\n",
    "            log_reg.fit(distances.reshape((-1, 1)), labels)\n",
    "            self.threshold = (-log_reg.intercept_ / log_reg.coef_).item()\n",
    "\n",
    "    # TODO refactor this method so we don't have to call .to(self.device) ? \n",
    "    def predict(self, images1, images2):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            images1 = images1.to(self.device)\n",
    "            images2 = images2.to(self.device)\n",
    "            distances = self.forward(images1, images2)\n",
    "            return (distances < self.threshold).int().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Create instances of the model and loaders\n",
    "    TODO move params batch_size, image_size, crop_size, paths to the separate cell\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(300),\n",
    "    T.CenterCrop(280),\n",
    "    T.Lambda(lambda x: T.functional.equalize(x)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "df_slices = list(split_dataframe(pd.read_csv('data/small.csv'), (0.9, 0.01, 0.04, 0.05)))\n",
    "datasets = [ImageDataset(df, transform=transform) for df in df_slices]\n",
    "loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=(i == 0)) for i, dataset in enumerate(datasets)]\n",
    "\n",
    "train_data, train_threshold_data, valid_data, test_data = df_slices\n",
    "train_dataset, train_threshold_dataset, valid_dataset, test_dataset = datasets\n",
    "train_loader, train_threshold_loader, valid_loader, test_loader = loaders\n",
    "\n",
    "submit_df = pd.read_csv('data/submit.csv')\n",
    "submit_dataset = ImageDataset(submit_df, transform=transform)\n",
    "submit_loader = DataLoader(submit_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = SiameseNetworkClassifier(latent_space_dim=100, dropout=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Train model\n",
    "\"\"\"\n",
    "\n",
    "train(model, *loaders, epochs=5, max_batches=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Check on what's wrong with our model\n",
    "\"\"\"\n",
    "\n",
    "mislabeled(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Save predictions for the submission\n",
    "\"\"\"\n",
    "\n",
    "max_submit_id = 22661\n",
    "save_submission(model, submit_loader, max_submit_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
